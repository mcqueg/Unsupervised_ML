{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Validity\n",
    "\n",
    "#### Garrett McCue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this notebook is to: \n",
    "1. Implement three cluster validity indices (one must be the CS index).\n",
    "2. Taking into consideration the results you obtained after implementing VAT, iVAT (as a pre-clustering technique) and FCM (as a clustering technique), compare the results you will get after applying the validity indices with the previous results of VAT (iVAT) and FCM. What do you think the best choice for the number of clusters (C) will be?\n",
    "\n",
    "\n",
    "\n",
    "The application of clustering algorithms to a dataset that does not have an optimal number of gropupings a prior can lead to the discovery of various amounts of underlying clusters. The issue with trying different numbers of clusters on the same dataset is that it can be difficult to discern what clustering is the best choice. The variation within cluster membership can be influenced by applying different clustering methods, or distance measures. The different clustering methods will still result with various clustering combinations, but still are without evidence of the best choice. In order to assess which clustering is the optimal choice, clustering validity indices can be computed by evaluating the \"goodness\" of the cluster set. The criterion for clustering goodness focuses on how well seprated each group is from one another and how compact or dense each grouping is. For a set of potential clustering numbers, or candidate parition set, the membership matrix, $\\cup$, for each candidate partion is evaluated using a validity index. Depending on the clustering index that is being used, the optimal choice will yield the min/max index value when compared to with all candidates. Classification entropy and Partition coefficient are two popular validity indices that are relatively simple to implement, using only the membership matrices for each candidate partition. The relatively simplistic nature of these algorithms comes with a trade off becasue the lack of direct connection with each data point of the clusterings themselves,  results in evaluation scores that do not assess the data in its entirety. Other validity indices such as CS index provide a more robust evaluation metric by considering the distance within each cluster and the distance between each cluster. The CS index algorithm is more complex to implement than both, Classification entropy and Partition coefficient algorithms, which is trade off when considering more complex criteria for clustering validity. The implementation and results of both simple algorithms as well as CS index will be evaluated on the [2022 World Happiness Dataset](https://www.kaggle.com/datasets/hemil26/world-happiness-report-2022) after [Clustering with FCM](https://nbviewer.org/github/mcqueg/Unsupervised_ML/blob/main/FCM.ipynb) and will be compared to the [VAT/iVAT](https://nbviewer.org/github/mcqueg/Unsupervised_ML/blob/main/VAT.ipynb) pre-clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fcmeans import FCM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>Country</th>\n",
       "      <th>Happiness score</th>\n",
       "      <th>Whisker-high</th>\n",
       "      <th>Whisker-low</th>\n",
       "      <th>Dystopia (1.83) + residual</th>\n",
       "      <th>Explained by: GDP per capita</th>\n",
       "      <th>Explained by: Social support</th>\n",
       "      <th>Explained by: Healthy life expectancy</th>\n",
       "      <th>Explained by: Freedom to make life choices</th>\n",
       "      <th>Explained by: Generosity</th>\n",
       "      <th>Explained by: Perceptions of corruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Finland</td>\n",
       "      <td>7.821</td>\n",
       "      <td>7.886</td>\n",
       "      <td>7.756</td>\n",
       "      <td>2.518</td>\n",
       "      <td>1.892</td>\n",
       "      <td>1.258</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>7.636</td>\n",
       "      <td>7.710</td>\n",
       "      <td>7.563</td>\n",
       "      <td>2.226</td>\n",
       "      <td>1.953</td>\n",
       "      <td>1.243</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>7.557</td>\n",
       "      <td>7.651</td>\n",
       "      <td>7.464</td>\n",
       "      <td>2.320</td>\n",
       "      <td>1.936</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>7.512</td>\n",
       "      <td>7.586</td>\n",
       "      <td>7.437</td>\n",
       "      <td>2.153</td>\n",
       "      <td>2.026</td>\n",
       "      <td>1.226</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.677</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>7.415</td>\n",
       "      <td>7.471</td>\n",
       "      <td>7.359</td>\n",
       "      <td>2.137</td>\n",
       "      <td>1.945</td>\n",
       "      <td>1.206</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK      Country  Happiness score  Whisker-high  Whisker-low  \\\n",
       "0     1      Finland            7.821         7.886        7.756   \n",
       "1     2      Denmark            7.636         7.710        7.563   \n",
       "2     3      Iceland            7.557         7.651        7.464   \n",
       "3     4  Switzerland            7.512         7.586        7.437   \n",
       "4     5  Netherlands            7.415         7.471        7.359   \n",
       "\n",
       "   Dystopia (1.83) + residual  Explained by: GDP per capita  \\\n",
       "0                       2.518                         1.892   \n",
       "1                       2.226                         1.953   \n",
       "2                       2.320                         1.936   \n",
       "3                       2.153                         2.026   \n",
       "4                       2.137                         1.945   \n",
       "\n",
       "   Explained by: Social support  Explained by: Healthy life expectancy  \\\n",
       "0                         1.258                                  0.775   \n",
       "1                         1.243                                  0.777   \n",
       "2                         1.320                                  0.803   \n",
       "3                         1.226                                  0.822   \n",
       "4                         1.206                                  0.787   \n",
       "\n",
       "   Explained by: Freedom to make life choices  Explained by: Generosity  \\\n",
       "0                                       0.736                     0.109   \n",
       "1                                       0.719                     0.188   \n",
       "2                                       0.718                     0.270   \n",
       "3                                       0.677                     0.147   \n",
       "4                                       0.651                     0.271   \n",
       "\n",
       "   Explained by: Perceptions of corruption  \n",
       "0                                    0.534  \n",
       "1                                    0.532  \n",
       "2                                    0.191  \n",
       "3                                    0.461  \n",
       "4                                    0.419  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "hap_df = pd.read_csv(\"data/world_happiness_rankings_2022.csv\")\n",
    "ranking_df = hap_df[['RANK', 'Country']]\n",
    "metrics_df = hap_df.drop(['RANK', 'Country'], axis=1)\n",
    "\n",
    "hap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "metrics_df = StandardScaler().fit_transform(metrics_df)\n",
    "\n",
    "# apply 2D PCA to data\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_data = pca_2.fit_transform(metrics_df)\n",
    "pca_2_df = pd.DataFrame(data=pca_2_data, columns=['PC1', 'PC2'])\n",
    "pca_2_ranking_df = pd.concat([ranking_df, pca_2_df], axis=1)\n",
    "\n",
    "# apply 3D PCA to data\n",
    "pca_3 = PCA(n_components=3)\n",
    "pca_3_data = pca_3.fit_transform(metrics_df)\n",
    "pca_3_df = pd.DataFrame(data=pca_3_data, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_3_ranking_df = pd.concat([ranking_df, pca_3_df], axis=1)\n",
    "\n",
    "X_2d = pca_2_df.to_numpy()\n",
    "X_3d = pca_3_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification entropy\n",
    "$N$ : number of data points  \n",
    "$\\mu_{ij}$ : the membership of the $i^\\text{th}$ data point to the $j^\\text{th}$  \n",
    "$C$ : number of cluster centers\n",
    "$$ CE(c) = -\\frac{1}{N}\\sum_{i=1}^{C}\\sum_{j=1}^{N}log(\\mu_{ij})\\mu_{ij}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce(u):\n",
    "    n, c = u.shape\n",
    "    return abs((np.log10(u) * u).sum() / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Clustering: \n",
      "classification entropy with 2 number of clusters is: 0.15636960306534034\n",
      "classification entropy with 3 number of clusters is: 0.26217202263157513\n",
      "classification entropy with 4 number of clusters is: 0.31092082979104\n",
      "classification entropy with 5 number of clusters is: 0.36385630813015407\n",
      "\n",
      "3D Clustering: \n",
      "classification entropy with 2 number of clusters is: 0.18630143251697812\n",
      "classification entropy with 3 number of clusters is: 0.29975887175241017\n",
      "classification entropy with 4 number of clusters is: 0.3707407382212827\n",
      "classification entropy with 5 number of clusters is: 0.4326485491159845\n"
     ]
    }
   ],
   "source": [
    "print(\"2D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_2d)\n",
    "    u = fcm.u\n",
    "    index = ce(u)\n",
    "\n",
    "    print(f\"classification entropy with {n_clusters} number of clusters is: {index}\")\n",
    "\n",
    "print(\"\\n3D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_3d)\n",
    "    u = fcm.u\n",
    "    index = ce(u)\n",
    "\n",
    "    print(f\"classification entropy with {n_clusters} number of clusters is: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Coefficient\n",
    "\n",
    "$N$ : number of data points  \n",
    "$\\mu_{ij}$ : the membership of the $i^\\text{th}$ data point to the $j^\\text{th}$  \n",
    "$C$ : number of cluster centers\n",
    "$$ PC(c) = \\frac{1}{N}\\sum_{i=1}^{C}\\sum_{j=1}^{N}\\mu_{ij}^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc(u):\n",
    "    n, c = u.shape\n",
    "    return np.square(u).sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Clustering: \n",
      "partition coeficient with 2 number of clusters is: 0.7781284291005209\n",
      "partition coeficient with 3 number of clusters is: 0.6568531487683938\n",
      "partition coeficient with 4 number of clusters is: 0.6264371762867962\n",
      "partition coeficient with 5 number of clusters is: 0.5800084653103927\n",
      "\n",
      "3D Clustering: \n",
      "partition coeficient with 2 number of clusters is: 0.7298308226247467\n",
      "partition coeficient with 3 number of clusters is: 0.6048014692255097\n",
      "partition coeficient with 4 number of clusters is: 0.5508124501907692\n",
      "partition coeficient with 5 number of clusters is: 0.5014878721381265\n"
     ]
    }
   ],
   "source": [
    "print(\"2D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_2d)\n",
    "    u = fcm.u\n",
    "    index = pc(u)\n",
    "\n",
    "    print(f\"partition coeficient with {n_clusters} number of clusters is: {index}\")\n",
    "\n",
    "print(\"\\n3D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_3d)\n",
    "    u = fcm.u\n",
    "    index = pc(u)\n",
    "\n",
    "    print(f\"partition coeficient with {n_clusters} number of clusters is: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS Index\n",
    "\n",
    "$C$ : number of cluster centers  \n",
    "$A_i$:  the number of data points in the cluster i\n",
    "$v_i$:  the cluster center of cluster i   \n",
    "$d_{ij}$ : Euclidean distance between $i^\\text{th}$ data point and $j^\\text{th}$ data point\n",
    "\n",
    "\n",
    "$$ CS(c) = \\frac{\\frac{1}{C}\\sum_{i=1}^{C} \\left\\{ \\frac{1}{|A_i|}\\sum_{\\vec{x_j}\\epsilon{A_i}}\\max_{\\vec{x_j}\\epsilon{A_i}}d(\\vec{x_i},\\vec{x_j}) \\right\\}}{\\frac{1}{C}\\sum_{i=1}^{C} \\left\\{ \\min_{j\\epsilon{C}_{j\\neq{i}}} d(\\vec{v_i},\\vec{v_j}) \\right\\}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(p,q):\n",
    "    if len(p) == 2:\n",
    "        distance = np.sqrt(((q[1]-p[1])**2)+((q[0]-p[0])**2))\n",
    "    elif len(p) == 3:\n",
    "        distance = np.sqrt(((q[1]-p[1])**2)+((q[0]-p[0])**2)+((q[2]-p[2])**2))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cs_index(X, centers, labels, n_clusters):\n",
    "    # list to hold the number of observations in each cluster\n",
    "    a = []\n",
    "    # initialize empty list to hold the max distance within each cluster\n",
    "    inner_max_dist_c = []\n",
    "    # loop through each cluster calculating the max within-cluster distance\n",
    "    for c in range(0, n_clusters):\n",
    "        # initialize list to hold the max within-distance for current cluster\n",
    "        inner_max_dist = []\n",
    "        # create temp array to hold only the observations for the current cluster\n",
    "        labels_c = np.squeeze(np.array(np.where(labels == c)))\n",
    "        # add the number of data points in cluster to list a\n",
    "        a.append(len(labels_c))\n",
    "        X_c = []\n",
    "        for k in labels_c:\n",
    "            X_c.append(X[k])\n",
    "        # loop through each observation within cluster saving distance of every sample vs all other samples\n",
    "        for row in range(len(X_c)):\n",
    "            # initialize list to hold all distances corresponding to the current observation\n",
    "            distances = []\n",
    "            # loop through all rows finding distance to current row\n",
    "            for j in range(len(X_c)):\n",
    "                # add distances to distances list for current row\n",
    "                distances.append(euclidean_dist(X_c[row], X_c[j]))\n",
    "            # find max distance for each sample vs all other samples\n",
    "            inner_max_dist.append(max(distances))\n",
    "        # find max distance for each clister\n",
    "        inner_max_dist_c.append(max(inner_max_dist))\n",
    "    # sum all max distances between points of each cluster\n",
    "    sum_inner_max_dist_c = sum(inner_max_dist_c)\n",
    "    # sum of max distances between points in each cluster divided by the size of each cluster\n",
    "    sum_a = 0\n",
    "    for i in range(len(a)):\n",
    "        sum_a = sum_a + ((1/a[i])*sum_inner_max_dist_c)\n",
    "    # cs_numerator: within cluster distance measures\n",
    "    cs_numerator = (1/n_clusters) * sum_a\n",
    "\n",
    "    # intitalize the sum of the between cluster distances as 0\n",
    "    sum_between_cluster_dist = 0\n",
    "    # loop through every cluster centroid to get the min distance between clsuter centroids\n",
    "    for c in range(len(centers)):\n",
    "        # initialize list to hold the distances between current cluster and all other clusters\n",
    "        centroid_dists = []\n",
    "        # find distances for each cluster centroid with current centroid\n",
    "        for j in range(len(centers)):\n",
    "            # add distance to list for current cluster\n",
    "            centroid_dists.append(euclidean_dist(centers[c], centers[j]))\n",
    "        # remove the distance measure of 0 (current cluster with itself)\n",
    "        centroid_dists.remove(0)\n",
    "        # add the min distance to the sum of between cluster distances\n",
    "        sum_between_cluster_dist = sum_between_cluster_dist + \\\n",
    "            (min(centroid_dists))\n",
    "    # cs_denominator: (1/n_clusters)*sum of between cluster distances\n",
    "    cs_denominator = (1/n_clusters)*sum_between_cluster_dist\n",
    "    # cs_index = cs_numerator/cs_denominator\n",
    "    index = cs_numerator/cs_denominator\n",
    "\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Clustering: \n",
      "cs index with 2 number of clusters is: 0.04421837449759678\n",
      "cs index with 3 number of clusters is: 0.12854420019334895\n",
      "cs index with 4 number of clusters is: 0.2357542140867573\n",
      "cs index with 5 number of clusters is: 0.38731534032540216\n",
      "\n",
      "3D Clustering: \n",
      "cs index with 2 number of clusters is: 0.04905895861605767\n",
      "cs index with 3 number of clusters is: 0.14109983855117003\n",
      "cs index with 4 number of clusters is: 0.2652773642084752\n",
      "cs index with 5 number of clusters is: 0.40310910586834714\n"
     ]
    }
   ],
   "source": [
    "print(\"2D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_2d)\n",
    "    centers_2d = fcm.centers\n",
    "    labels_2d = fcm.predict(X_2d)\n",
    "    index = cs_index(X_2d, centers=centers_2d, labels=labels_2d, n_clusters=n_clusters)\n",
    "\n",
    "    print(f\"cs index with {n_clusters} number of clusters is: {index}\")\n",
    "\n",
    "print(\"\\n3D Clustering: \")\n",
    "for n_clusters in range(2, 6):\n",
    "    fcm = FCM(n_clusters=n_clusters, m=2)\n",
    "    fcm.fit(X_3d)\n",
    "    centers_3d = fcm.centers\n",
    "    labels_3d = fcm.predict(X_3d)\n",
    "    index = cs_index(X_3d, centers=centers_3d, labels=labels_3d, n_clusters=n_clusters)\n",
    "\n",
    "    print(f\"cs index with {n_clusters} number of clusters is: {index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Validity Analysis\n",
    "\n",
    "#### Classification Entropy\n",
    "When applying the Classification Entropy to the 2D and 3D PCA projected data, the optimal number of clusters is given by the lowest index value for the number of clusters. The lowest classification entropy of the 2D data was 0.156, resulting in the optimal number of clusters being 2. The classifcation entropy of the 3D data also resulted in the optimal clsutering number of 2, with a minimum index value of 0.186. When comparing the two index values it seems that 2 clusters within the 2D dataset is a better clustering than 2 clusters within the 3D dataset. \n",
    "\n",
    "#### Partition Coefficient \n",
    "When applying the Partition Coefficient to the 2D and 3D PCA projected data, the optimal number of clusters is given by the largest index value for the number of clusters. The largest partition Coefficient of the 2D data was 0.778, resulting in the optimal number of clusters being 2. The Partition Coefficient of the 3D data also resulted in the optimal clsutering number of 2, with a max index value of 0.729. When comparing the two index values it seems that 2 clusters within the 2D dataset is a better clustering than 2 clusters within the 3D dataset. \n",
    "\n",
    "#### CS Index\n",
    "When applying the CS Index to the 2D and 3D PCA projected data, the optimal number of clusters is given by the lowest index value for the number of clusters. The lowest CS Index of the 2D data was 0.044, resulting in the optimal number of clusters being 2. The CS Index of the 3D data also resulted in the optimal clsutering number of 2, with a minimum index value of 0.049. When comparing the two index values it seems that 2 clusters within the 2D dataset is a better clustering than 2 clusters within the 3D dataset. \n",
    "\n",
    "#### Comparison with VAT/iVAT Analysis\n",
    "When the VAT/iVAT algorithms were applied to the 2D and 3D projected data the analysis resulted in images suggesting the presence of 3, 4, and potentially 5 clusters. The FCM clustering algorithm was applied to the 2D and 3D projected data searching for the optimal number of clusters based on the suggested clustering numbers generated by the preclustering analysis. When the cluster validity indices were computed for all possible clusterings from the FCM algorithm it was determined that a smaller number of clusters were optimal for this dataset. The indices were also computed for a clustering with 2 groups, which was not the suggested clustering number assessed from the VAT/iVAT algorithm. This can serve as a reminder that the pre-clustering analysis is only a guide for the datasets ability to cluster and is not a definitive guide for the optimal clustering number. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "935f013ec3274d983f6cf9c6c8d3a038a9827a62579d2563240b276044aef7b2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
